{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1530, 2720, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read data in\n",
    "#file = np.load('output.npz')\n",
    "#xIn = file['x']\n",
    "#yIn = file['y']\n",
    "#print(xIn.shape)\n",
    "imageShape = (1530,2720,3)\n",
    "print(imageShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 1530, 2720, 3), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# setup the place holder input for the images\n",
    "imageWidth = imageShape[1]\n",
    "imageHeight = imageShape[0]\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, imageHeight, imageWidth, imageShape[2]])\n",
    "\n",
    "# setup placeholder input for labels\n",
    "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "# placeholder for batch norm training phase.\n",
    "trainPhase = tf.placeholder(tf.bool)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch norm function for use.\n",
    "decayRate = 0.99\n",
    "betaInit = tf.zeros_initializer(dtype=tf.float32)\n",
    "gammaInit = tf.ones_initializer(dtype=tf.float32)\n",
    "\n",
    "# batchNormLayer\n",
    "# Adds a batch normalization layer to to the filter.\n",
    "# x - input tensor\n",
    "# filterShape - shape of filter\n",
    "# num - the number to not have the same variable name for the gamma and beta variables.\n",
    "# filtType - the type of filter (conv, mult)\n",
    "def batchNormLayer(x, numChannels, num, filtType='conv'):\n",
    "    # assumed to be convlution filter\n",
    "\n",
    "    #define weight variables\n",
    "    gamma = tf.get_variable('gamma' + str(num), [numChannels], initializer=gammaInit)\n",
    "    beta = tf.get_variable('beta' + str(num), [numChannels], initializer=betaInit)\n",
    "\n",
    "    axes = []\n",
    "    if filtType == 'mult':\n",
    "        axes = [0]\n",
    "    else:\n",
    "        axes = [0,1,2]\n",
    "    \n",
    "    batch_mean, batch_variance = tf.nn.moments(x, axes)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay=decayRate)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_variance])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_variance)\n",
    "\n",
    "    mean, variance = tf.cond(trainPhase,\n",
    "                        mean_var_with_update,\n",
    "                        lambda: (ema.average(batch_mean), ema.average(batch_variance)))\n",
    "    \n",
    "    \n",
    "\n",
    "    normed = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.000001)\n",
    "    return normed, gamma, beta, ema.average(batch_mean), ema.average(batch_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numConvLayers = 0\n",
    "# Define variable initilization\n",
    "normInit = tf.truncated_normal_initializer(0,.05, dtype=tf.float32)\n",
    "zeroInit = tf.constant_initializer(0.05, dtype=tf.float32)\n",
    "\n",
    "# convLayer\n",
    "# define convolutional layer with batch normalization, and max pooling\n",
    "# @param x - the input tensor\n",
    "# @param filterShape - the shape of the filter (height, width, num channels output)\n",
    "# @param poolShape - the shape of the pooling (height width)\n",
    "#\n",
    "# @return layer, list of all variables\n",
    "def convLayer(x, filterShape, poolShape):\n",
    "    global numConvLayers\n",
    "    inputChannels = x.shape[3]\n",
    "    convFilt = tf.get_variable('filt' + str(numConvLayers), \\\n",
    "        [filterShape[0], filterShape[1], inputChannels, filterShape[2]], \\\n",
    "        initializer=normInit)\n",
    "    bias = tf.get_variable('bias' + str(numConvLayers), \\\n",
    "        [filterShape[2]], initializer=zeroInit)\n",
    "    \n",
    "    # setup layer conv, batch norm, and pooling layers.\n",
    "    logit = tf.nn.conv2d(x, convFilt, strides=[1,1,1,1], padding='SAME') + bias\n",
    "    normed, gamma, beta, mean, variance = \\\n",
    "        batchNormLayer(logit, filterShape[2], numConvLayers)\n",
    "    layer = tf.nn.relu(normed)\n",
    "    pooled = tf.nn.max_pool(layer, \\\n",
    "                ksize=[1,poolShape[0], poolShape[1], 1], \\\n",
    "                strides=[1,poolShape[0], poolShape[1], 1], \\\n",
    "                padding='SAME')\n",
    "    \n",
    "    \n",
    "    numConvLayers += 1\n",
    "    return pooled, [convFilt, bias, gamma, beta, mean, variance]\n",
    "    \n",
    "\n",
    "# fullConnLayer\n",
    "# Define a fully connected layer with batch normalization\n",
    "# @param x - the input tensor\n",
    "# @param numOutputNodes - number of output nodes\n",
    "#\n",
    "# @return outputLayer, list of all variables\n",
    "def fullConnLayer(x, numOutputNodes):\n",
    "    global numConvLayers\n",
    "    inputChannels = x.shape[1]\n",
    "    \n",
    "    matFilt = tf.get_variable('filt' + str(numConvLayers), \\\n",
    "        [inputChannels, numOutputNodes], initializer=normInit)\n",
    "    bias = tf.get_variable('bias' + str(numConvLayers), \\\n",
    "        [numOutputNodes], initializer=zeroInit)\n",
    "    \n",
    "    logit = tf.matmul(x, matFilt) + bias\n",
    "    normed, gamma, beta, mean, variance = \\\n",
    "        batchNormLayer(logit, numOutputNodes, numConvLayers, 'mult')\n",
    "    layer = tf.nn.relu(normed)\n",
    "        \n",
    "    numConvLayers += 1\n",
    "    return layer, [matFilt, bias, gamma, beta, mean, variance]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 765, 1360, 128), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 153, 272, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 51, 91, 128), dtype=float32)\n",
      "Tensor(\"MaxPool_3:0\", shape=(?, 17, 31, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_4:0\", shape=(?, 9, 16, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_5:0\", shape=(?, 5, 8, 32), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 1280), dtype=float32)\n",
      "Tensor(\"Relu_6:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"Relu_7:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"add_8:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "variables = []\n",
    "\n",
    "# define inference\n",
    "layer1, tmp = convLayer(x, [3,3,128], [2,2])\n",
    "print(layer1)\n",
    "variables += tmp\n",
    "\n",
    "layer2, tmp = convLayer(layer1, [5,5,64], [5,5])\n",
    "variables += tmp\n",
    "print(layer2)\n",
    "\n",
    "layer3, tmp = convLayer(layer2, [3,3,128], [3,3])\n",
    "variables += tmp\n",
    "print(layer3)\n",
    "\n",
    "layer4, tmp = convLayer(layer3, [7,7,64], [3,3])\n",
    "variables += tmp\n",
    "print(layer4)\n",
    "\n",
    "layer5, tmp = convLayer(layer4, [5,5,64], [2,2])\n",
    "variables += tmp\n",
    "print(layer5)\n",
    "\n",
    "layer6, tmp = convLayer(layer5, [7,7,32], [2,2])\n",
    "variables += tmp\n",
    "print(layer6)\n",
    "\n",
    "sh = layer6.shape\n",
    "flattened = tf.reshape(layer6, shape=[-1, sh[1]*sh[2]*sh[3]])\n",
    "print(flattened)\n",
    "\n",
    "full1, tmp = fullConnLayer(flattened, 1024)\n",
    "variables += tmp\n",
    "print(full1)\n",
    "\n",
    "full2, tmp = fullConnLayer(flattened, 1024)\n",
    "variables += tmp\n",
    "print(full2)\n",
    "\n",
    "outputMat = tf.get_variable('outputMat', [full2.shape[1], 2], initializer=normInit)\n",
    "outputBias = tf.get_variable('outputBias', [2], initializer=zeroInit)\n",
    "\n",
    "outputLogit = tf.matmul(full2, outputMat) + outputBias\n",
    "print(outputLogit)\n",
    "variables = variables + [outputMat, outputBias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tensorflow saver\n",
    "saver = tf.train.Saver(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "crossEntropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=outputLogit)\n",
    "loss = tf.reduce_mean(crossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## define training step\n",
    "\n",
    "learningRate = 0.001\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate)\n",
    "trainStep = optimizer.minimize(loss)\n",
    "\n",
    "###### define accuracy functions\n",
    "\n",
    "# output of equals is integers\n",
    "actualClass = tf.argmax(y, axis=1)\n",
    "predictedClass = tf.argmax(outputLogit, axis=1)\n",
    "\n",
    "equals = tf.equal(actualClass, predictedClass)\n",
    "\n",
    "# cast integers to float for reduce mean to work correctly.\n",
    "accuracy = tf.reduce_mean(tf.cast(equals, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ian/anaconda/envs/flow/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/Ian/anaconda/envs/flow/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1530, 2720, 3)\n",
      "(30, 2)\n",
      "(30, 1530, 2720, 3)\n"
     ]
    }
   ],
   "source": [
    "# define batch code\n",
    "# next_batch function will return a random set of images loaded from the given feed directory\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as im\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import labelReader\n",
    "\n",
    "# read in all image names\n",
    "feed_dir = '../labelingTool/feed/'\n",
    "imageFilenames = glob.glob(feed_dir + '*.jpg')\n",
    "# read in labels csv file\n",
    "labelsFromFilenames = labelReader.readLabelsDict('../labelingTool/labels.csv')\n",
    "\n",
    "############################################## Variables to set batch sizes\n",
    "numValidationImages = 50\n",
    "batchSize = 30\n",
    "#############################################\n",
    "\n",
    "# pull out small subset of images to use for validation\n",
    "validationFilenames = imageFilenames[0:numValidationImages]\n",
    "trainFilenames = imageFilenames[numValidationImages:]\n",
    "\n",
    "\n",
    "\n",
    "# this function goes through and pulls random\n",
    "# parts of the training data out\n",
    "# @param filenames - a list of all possible filenames to pull from\n",
    "# @param batchSize - the list of batchSize \n",
    "#\n",
    "# @return - images, labels as numpy arrays\n",
    "def pullRandomBatch(filenames, batchSize):\n",
    "    # select random indcies of filenames\n",
    "    indicies = np.random.choice(len(filenames), batchSize, replace=False)\n",
    "    \n",
    "    # Read in all filenames of \n",
    "    batchFilenames = [filenames[i] for i in indicies]\n",
    "    \n",
    "    return createBatch(batchFilenames)\n",
    "    \n",
    "\n",
    "def createBatch(batchFilenames):\n",
    "    # create vector of images and labels\n",
    "    images = np.empty((batchSize, imageShape[0], imageShape[1], imageShape[2]))\n",
    "    labels = np.zeros((batchSize, 2))\n",
    "    \n",
    "    # Go through each image filename and read in the image, and put\n",
    "    # it into a label.\n",
    "    # If the the filesize is not correct, than resize the image.\n",
    "    for i in range(len(batchFilenames)):\n",
    "        image = im.imread(batchFilenames[i])\n",
    "        \n",
    "        # resize image if wrong size.\n",
    "        if image.shape != imageShape:\n",
    "            image = resize(image, imageShape)\n",
    "        \n",
    "        # set image into numpy array\n",
    "        images[i] = image\n",
    "        filenameBase = os.path.basename(batchFilenames[i])\n",
    "        \n",
    "        # set label for the image.\n",
    "        label = labelsFromFilenames[filenameBase]\n",
    "        if label == 'noPeople':\n",
    "            labels[i][0] = 1.0\n",
    "        elif label == 'people':\n",
    "            labels[i][1] = 1.0\n",
    "        else:\n",
    "            print('Error, the image: '+ filename + 'is not in the labels')\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "train, label = pullRandomBatch(trainFilenames, batchSize)\n",
    "print(train.shape)\n",
    "print(label.shape)\n",
    "\n",
    "validData, labelLabels = createBatch(validationFilenames)\n",
    "print(validData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# comment the restore call and uncoment the global initializer to restart the training the process.\n",
    "#saver.restore(sess, 'V6')\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ian/anaconda/envs/flow/lib/python3.5/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Users/Ian/anaconda/envs/flow/lib/python3.5/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "# define training loop\n",
    "numIterations = 2000\n",
    "numToValidate = 50\n",
    "numToSave = 1000000\n",
    "\n",
    "\n",
    "\n",
    "lossSum = 0.0\n",
    "for i in range(numIterations):\n",
    "    train, label = pullRandomBatch(trainFilenames, batchSize)\n",
    "    feed = {x: train, y: label, trainPhase: True}\n",
    "    \n",
    "    lossSum += sess.run(loss, feed_dict=feed)\n",
    "    sess.run(trainStep, feed_dict=feed)\n",
    "    \n",
    "    if (i+1) % numToSave == 0: \n",
    "        # save the current model\n",
    "        #saver.save(sess, 'layer2/layer2ShortFilters', global_step=i)\n",
    "        xyz =1\n",
    "    \n",
    "    if i % numToValidate == 0: \n",
    "        feed = {x: validData, y: labelLabels, trainPhase: False}\n",
    "        \n",
    "        acc = sess.run(accuracy, feed_dict=feed)\n",
    "        print('iteration num: ' + str(i))\n",
    "        print('Validation accuracy = ' + str(acc))\n",
    "        print('Avg loss = ' + str(lossSum / numToValidate))\n",
    "        lossSum = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:flow]",
   "language": "python",
   "name": "conda-env-flow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
